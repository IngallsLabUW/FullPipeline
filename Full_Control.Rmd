Add working function for specific compound checks (lines ~121)
Why the specific mz of noise_mz <- 123.04054? And why the syntax/mz of split_mz <- 83.037114+1.007276?
Add working function for specific pooled msdata checks (lines ~185)
Read more about RT corrections
What does the 3 groups when bw < 12, 1 group if bw >12 mean?
Addiso envelopes doesn't seem to be defined.
Addiso peaks also not defined.
BMIS plotting spotty because of missing columns.
As far as I can tell, Sirius (in this setup) doesn't like running on Macs. I will investigate this later.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, error = TRUE)
options(dplyr.summarise.inform=F)
library(anytime)
library(data.table)
library(pbapply)
library(plotly)
library(RaMS)
library(rlist)
library(tidyr)
library(tidyverse)
library(xcms)

options(scipen=999)
currentDate <- Sys.Date()

source("targeted_functions.R")
source("untargeted_functions.R")
```

```{r, make_folders, include=TRUE}

# Run information
file_pattern <- "meso"
instrument_pattern <- "TQS"
targ_software_pattern <- "Skyline"
polarity <- "positive"
column <- "HILIC"

cruise <- "mesocenter"
output_folder <- paste0(c(cruise, polarity, "output/"), collapse = "_")
mzml_path <- paste0("../untargeted/mzMLs/", polarity, "/")


# Create folder if it doesn't already exist
if(!dir.exists(output_folder)) {dir.create(output_folder)}
```

Start by acquiring all files. 
Targeted
```{r targeted_files, include = TRUE}


source("../Targeted_Pipeline/src/File_Import.R")
```

```{r Dataset rearrangement, include = TRUE}
## Skyline HILIC variables: 
skyline.HILIC.neg <- X181116_HILIC_neg_MESO.SCOPE_HRM
skyline.HILIC.pos <- X181116_HILIC_pos_MESO.SCOPE_HRM

source("../Targeted_Pipeline/src/Skyline_Rearrange.R")
```


Untargeted
```{r untargeted_files, include=TRUE}

# Load required standards data, clean
given_stans <- read.csv("../untargeted/metadata/clean_stans.csv")

# Load required metadata, clean
cruise_metadata <- read.csv("../untargeted/metadata_complete.csv") %>%
  filter(cruise==!!cruise)

run_date <- cruise_metadata %>%
  filter(cruise==!!cruise) %>%
  pull(date_run) %>%
  unique()

ms_files <- cruise_metadata$filename

BiocParallel::register(BPPARAM = BiocParallel::SnowParam(tasks = length(ms_files), progressbar = TRUE))
```

## Peakpicking

Select peaks using XCMS's centWave peakpicking.
Calculate improved signal to noise.
Calculate normality (Gaussian-ness).
Return list of paths to .mzML and large df full of peaks.

```{r peakpicking params, error=TRUE}
# Define the peakpicking parameters
cwp <- CentWaveParam(ppm = 2.5, peakwidth = c(15, 15), 
                     snthresh = 1, prefilter = c(5, 10000), 
                     integrate = 2, mzCenterFun = "wMean", 
                     mzdiff = 0.001, fitgauss = FALSE, 
                     noise = 5000, firstBaselineCheck = FALSE, 
                     extendLengthMSW = TRUE)

# Set the new quality threshold
qscore_threshold <- 20

# Define the retention time correction parameters
obp <- ObiwarpParam(binSize = 0.1, centerSample = 27, 
                    response = 1, distFun = "cor_opt")

# Define the correspondence parameters
pdp <- PeakDensityParam(sampleGroups = cruise_metadata$depth, 
                        bw = 12, minFraction = 0.1, 
                        binSize = 0.001, minSamples = 2)

# Make sure that filled peaks have at least a 2.5ppm window from mzmin to mzmax around mz
fpp <- FillChromPeaksParam(ppm = 2.5)
```

Do actual peakpicking
```{r peakpicking, include=TRUE}

# Perform peakpicking
source("../untargeted/scripts/peakpicking.R")

# Save intermediate results
saveRDS(xdata, file = paste0(output_folder, "xdata.rds"))
saveRDS(xdata_cleanpeak, file = paste0(output_folder, "xdata_cleanpeak.rds"))
saveRDS(xdata_rt, file = paste0(output_folder, "xdata_rt.rds"))
saveRDS(xdata_cor, file = paste0(output_folder, "xdata_cor.rds"))
saveRDS(xdata_filled, file = paste0(output_folder, "xdata_filled.rds"))
write.csv(raw_peaks, file = paste0(output_folder, "raw_peaks.csv"), row.names = FALSE)
unique(warnings())
```

Targeted Quality Control
```{r QC parameters, include = TRUE}
# QE + TQS QC parameters
area.min   <- 1000 # HILIC - 1000, Cyano - 5000
RT.flex    <- 0.4 # HILIC +/- 0.4 min, Cyano +/- 0.2 min 
blk.thresh <- 0.3 # HILIC +/- 0.3, Cyano +/- 0.2
SN.min     <- 4 # HILIC - 4, Cyano - 4
height.min <- 1000
height.max <- 1.0e8

# Additional QC parameters for Skyline TQS 
# Comment this out when using MSDial
area.max <- 1.0e8
IR.flex  <- 0.3
ppm.flex <- 7

source("src/Skyline_QC.R")  
```

Save any required output here to an intermediate folder
```{r, include = TRUE}
currentDate <- Sys.Date()
csvFileName <- paste("data_processed/", software.pattern, "_", instrument.pattern,
                     "_QC_Output_", file.pattern, "_", currentDate, ".csv", sep = "")

tables <- grep("table", names(.GlobalEnv), value = TRUE, ignore.case = TRUE)
tablelist <- do.call("list", mget(tables))

# Write intermediate data
invisible(lapply(tables, 
                 function(x) write.csv(get(x), file=paste("data_intermediate/",
                                                            software.pattern, "_",
                                                          instrument.pattern,
                                                            "_", x, "_", currentDate,
                                                          ".csv", sep = ""))))
# Write final data
write.csv(final.table, csvFileName, row.names = FALSE)

print(paste(tables, "saved to data/intermediate"))
```


Isolate and correct the Untargeted pooled data.
```{r pooled_msdata}
xdata_filled <- readRDS(file = paste0(output_folder, "xdata_filled.rds"))

pooled_idx <- grep(fileNames(xdata_filled), pattern = "Poo") %>%
  sprintf(fmt = "F%02d") %>%
  paste0(collapse = "|")
xdata_temp <- dropAdjustedRtime(xdata_filled)
pooled_rtimes <- rtime(xdata_temp) %>%
  `[`(., grepl(names(.), pattern = pooled_idx))
corrected_rtimes <- rtime(xdata_filled) %>%
  `[`(., grepl(names(.), pattern = pooled_idx))
names(corrected_rtimes) <- round(pooled_rtimes, digits = 5)

pooled_msdata <- grep(ms_files, pattern = "Poo", value = TRUE) %>%
  paste0(mzml_path, .) %>%
  grabMSdata(grab_what = c("MS1", "BPC"))

pooled_msdata$MS1$rt_cor <- corrected_rtimes[as.character(round(pooled_msdata$MS1$rt*60, digits = 5))]

saveRDS(pooled_msdata, file = paste0(output_folder, "pooled_msdata_", polarity, ".rds"))
```

Check for specific peaks here.
```{r peakpickcheck}
# Read in raw data
xdata_filled <- readRDS(file = paste0(output_folder, "xdata_filled.rds"))
pooled_msdata <- readRDS(paste0(output_folder, "pooled_msdata_", polarity, ".rds"))

# Check on number of peaks picked for Alanine (or other specific compounds)
ala_mz <- given_stans %>% filter(compound_name=="L-Alanine") %>% pull(mz)
featureDefinitions(xdata_filled) %>%
  as.data.frame() %>%
  filter(mzmed%between%pmppm(ala_mz, 10)) %>%
  ggplot() + 
  geom_vline(aes(xintercept = rtmin), color="green") +
  geom_vline(aes(xintercept = rtmax), color="red") +
  geom_line(aes(x=rt_cor, y=int, group=filename), 
            data = pooled_msdata$MS1[mz%between%pmppm(ala_mz)])

# Number of peaks picked for noise
noise_mz <- 123.04054
featureDefinitions(xdata_filled) %>%
  as.data.frame() %>%
  filter(mzmed%between%pmppm(noise_mz, 10)) %>%
  ggplot() + 
  geom_vline(aes(xintercept = rtmin), color="green") +
  geom_vline(aes(xintercept = rtmax), color="red") +
  geom_line(aes(x=rt_cor, y=int, group=filename), 
            data = pooled_msdata$MS1[mz%between%pmppm(noise_mz)])

# Check on npeaks for unknown split peak
split_mz <- 83.037114+1.007276
featureDefinitions(xdata_filled) %>%
  as.data.frame() %>%
  filter(mzmed%between%pmppm(split_mz, 10)) %>%
  ggplot() + 
  geom_vline(aes(xintercept = rtmin), color="green") +
  geom_vline(aes(xintercept = rtmax), color="red") +
  geom_line(aes(x=rt_cor, y=int, group=filename), 
            data = pooled_msdata$MS1[mz%between%pmppm(split_mz, 10)])
```

Check the retention times and perform corrections.
```{r rtcheck}
cairo_pdf(filename = paste0(output_folder, "rt_cor_plot.pdf"))
plotAdjustedRtime(xdata_filled, col = hcl.colors(length(unique(cruise_metadata$depth)), alpha = 0.2)[
  factor(cruise_metadata$depth)])
legend("bottomleft", legend = unique(cruise_metadata$depth), 
       col = hcl.colors(length(unique(cruise_metadata$depth))),
       lty = 1, bty="n", ncol = 3)

internal_stans <- given_stans %>% 
  filter(compound_type=="Internal Standard") %>%
  filter(polarity==!!polarity)

groupdata <- xdata_filled %>%
  featureDefinitions() %>%
  as.data.frame() %>%
  as_tibble()
internal_stans %>%
  pmap_dfr(function(...){
    row_data <- tibble(...)
    filter(groupdata, mzmed%between%pmppm(row_data$mz, 10)) %>% 
      mutate(compound_name = row_data$compound_name)
      #cbind(row_data$compound_name, .) # This is the original code: it adds IS name and checks that an IS       was found. The error is a handling exceptions for when nothing is there.
  }) %>%
  as_tibble()

# Check any specific pooled data for RT corrections
pooled_msdata$MS1[mz%between%pmppm(90.055504, 10)] %>%
  mutate(rt=rt*60) %>%
  pivot_longer(cols = c("rt", "rt_cor"), names_to = "rt_type") %>% 
  ggplot() + 
  geom_line(aes(x=value, y=int, group=filename, color=rt_type)) +
  scale_color_manual(values = c("#FF000044", "#00FF0044")) +
  facet_wrap(~rt_type, ncol = 1) +
  xlim(500, 750)

# This one file shows some weird RT correction
## Corrected RT:
pooled_msdata$MS1[mz%between%pmppm(93.07429, 10)] %>%
  ggplot(aes(x=rt_cor, y=int, group=filename)) +
  geom_line() +
  geom_point() +
  facet_wrap(~filename, ncol = 3)
## Normal RT:
pooled_msdata$MS1[mz%between%pmppm(93.07429, 10)] %>%
  ggplot(aes(x=rt, y=int, group=filename)) +
  geom_line() +
  geom_point() +
  facet_wrap(~filename, ncol = 3)
```

Grouped retention time correction checks
```{r groupcheck}
pooled_file_idxs <- grep("Poo", fileNames(xdata_filled))
xdata_pooled <- filterFile(xdata_filled, pooled_file_idxs)
xdata_pooled_rtcor <- xdata_pooled
new_rts <- adjustedRtime(xdata_filled)[fromFile(xdata_filled)%in%pooled_file_idxs]
adjustedRtime(xdata_pooled_rtcor) <- split(new_rts, str_extract(names(new_rts), "F\\d+"))
chr_sampleGroups <- str_extract(fileNames(xdata_pooled), "180821|180205|190715")
pdp <- PeakDensityParam(sampleGroups = chr_sampleGroups, 
                        bw = 12.5, minFraction = 0.1, 
                        binSize = 0.001, minSamples = 2)

# Check some specific mzs, can define multiple ways
ala_mz <- given_stans %>% filter(compound_name=="L-Alanine") %>% pull(mz)
# 3 groups when bw < 12
# 1 group if bw >12
featureDefinitions(xdata_filled) %>%
  as.data.frame() %>%
  filter(mzmed%between%pmppm(ala_mz, 5)) %>%
  `rownames<-`(NULL) %>% 
  arrange(rtmed)
par(mfrow=c(2,1))
raw_chr <- chromatogram(xdata_pooled, mz=pmppm(ala_mz), rt=c(550, 750))
plot(raw_chr, col=NA)
raw_chr_cor <- chromatogram(xdata_pooled_rtcor, mz=pmppm(ala_mz), rt=c(550, 750))
plot(raw_chr_cor, col=NA)

grp_chr <- groupChromPeaks(raw_chr_cor, pdp)
featureDefinitions(grp_chr)
plotChromPeakDensity(grp_chr, col=hcl(c(120, 240, 360))[factor(chr_sampleGroups)])
```

## De-isotoping and de-adducting

After peaks have been identified, remove the isotopes and adducts of other peaks.
```{r deisoadduct params, error=TRUE}
raw_peaks <- read.csv(file = paste0(output_folder, "raw_peaks.csv"))

not_addisos <- list("Glutamine"=c(mz=147.076968, rt=620),
                    "Citrulline"=c(mz=176.103517, rt=645),
                    "Guanine"=c(mz=152.0567, rt=400),
                    "Glutamic acid"=c(mz=148.061, rt=745),
                    "4-Aminobutyric acid"=c(mz=104.071, rt=614))
# How many seconds away from the given RT can the peak be before it's removable?
# Set high to collect entire EIC
peak_rt_flex <- 500

# When removing peaks that are likely adducts...
# How similar do the median peak and median adduct need to be to assume adduct?
shape_remove_threshold <- 0.8
# How good does the peak area ~ adduct area correlation across files need to be to assume adduct?
area_remove_threshold <- 0.99

# When finding adducts and isotopes of a given peak...
# How similar do the median peak and median adduct need to be to assume adduct?
# Typically lower than above because priors are better
shape_find_threshold <- 0.75
# How good does the peak area ~ adduct area correlation across files need to be to assume adduct?
area_find_threshold <- 0.9
```

```{r deistope, include=TRUE}
source("../untargeted/scripts/deisoadduct.R")

# Save intermediate files
saveRDS(is_peak_iso, file = paste0(output_folder, "is_peak_iso.rds"))
saveRDS(peak_envelopes, file = paste0(output_folder, "peak_envelopes.rds"))

# Write out data frame containing features that are likely adducts and isotopes
write.csv(addiso_features, file = paste0(output_folder, "addiso_features.csv"), row.names = FALSE)

# Write out data frame containing decomposed isotope/adduct envelopes for non-adduct/iso features
# Average MS1 peak areas for each feature and its isotopes if they pass the above thresholds
write.csv(feature_envelopes, file = paste0(output_folder, "feature_envelopes.csv"), row.names = FALSE)

# Write out raw_peaks but it's been filled in and M_area has been calculated for every single peak
# Includes ALL peaks, both addiso and not
write.csv(filled_peaks, file = paste0(output_folder, "filled_peaks.csv"), row.names = FALSE)

unique(warnings())
```

Check the isotopes and adducts.
```{r isoaddcheck}
saveRDS(pooled_msdata, file = paste0(output_folder, "pooled_msdata_", polarity, ".rds"))
raw_peaks <- read.csv(paste0(output_folder, "raw_peaks.csv"))
addiso_features <- read.csv(paste0(output_folder, "addiso_features.csv"))
iso_masses <- c(C13=1.003355, X2C13=2*1.003355, S34=1.995796, S33=0.999387, N15=0.997035, O18=2.004244)
adduct_masses <- c(Na=22.98922-1.007276, NH4=18.0338-1.007276, H2O_H=-18.0106, K=38.963708-1.007276)
addiso_masses <- c(adduct_masses, iso_masses)

# Convert to feature-based format
# How many adducts/isos of each type were found?
# Check on some random adduct-iso pairings

raw_peaks %>%
  group_by(feature) %>%
  filter(duplicated(filename)) %>%
  select(feature, filename) %>%
  left_join(raw_peaks, by=c("feature", "filename")) %>%
  select(feature, filename, mz) %>%
  as.data.frame() %>%
  slice(1:20)

grabMSdata(files = "../untargeted/mzMLs/pos/180821_Smp_MS7C115m_B.mzML", grab_what =
"MS1")$MS1[mz%between%pmppm(63.02701423)] %>%
  ggplot(aes(x=rt, y=int)) + geom_point() + geom_line() + 
  facet_wrap(~filename, ncol = 1) + xlim(c(9.5, 10.5))
```


## Annotate standards, untargeted
```{r targeted}
filled_peaks <- read.csv(paste0(output_folder, "filled_peaks.csv"))
given_stans <- read.csv("../untargeted/metadata/clean_stans.csv") %>% 
  filter(polarity==!!polarity) %>%
  filter(run_date>=date_added)

feature_data <- filled_peaks %>%
  group_by(feature) %>%
  summarise(mzmed=median(mz), rtmed=median(rt), avgarea=mean(M_area))

source("../untargeted/scripts/standard_assignments.R")

write.csv(stan_annotations, paste0(output_folder, "stan_annotations.csv"), 
          row.names = FALSE)
```

## Here, should we run phobos for targeted information?

## B-MIS, Targeted
```{r BMIS cutoff values, include = TRUE}
cut.off <- 0.4 
cut.off2 <- 0.1 

print("B-MIS Cutoff values assigned.")

# Skyline
software.pattern <- "Skyline"

# HILIC Skyline
Column.Type = "HILIC"
standards.pattern = "Ingalls"
QC.pattern = "QC_Output_HILIC"
```

Import required files.
```{r BMIS imports, include=TRUE}
source("src/BMIS_Imports.R")
```

If data is HILIC, identify duplicates and decide which to remove 
IdentifyDuplicates function will confirm if instrument column data exists.
User will need to use best judgement to decide which duplicate to remove.
```{r Check duplicates, include=TRUE}
source("src/Check_Duplicates.R")
currentDate = Sys.Date()
csvFileName <- paste("data_intermediate/", software.pattern, 
                     "_HILIC.duplicates_", currentDate, ".csv", sep = "")

# Using the duplicates.testing table, decide which detected compound to keep, the positive or negative.

if (exists("duplicates.testing") == TRUE) {
  print("This is a HILIC run. Look at duplicates.testing to decide which compounds to remove.")
} else {
  print("Non-HILIC data: no duplicates to remove.")
}

```

Here we sometimes remove any duplicates. 

Run BMIS.
```{r BMIS, include=TRUE}
source("src/BMIS.R")
```


## Finding B-MIS, Untargeted
```{r B-MIS}
filled_peaks <- read.csv(paste0(output_folder, "filled_peaks.csv"))
stan_annotations <- read.csv(paste0(output_folder, "stan_annotations.csv"))
given_stans <- read.csv("../untargeted/metadata/clean_stans.csv") %>% 
  filter(polarity==!!polarity)

min_improvement <- 0.4
already_good <- 0.1 #Not currently used

source("../untargeted/scripts/bmisscript.R")

write.csv(IS_peaks, file = paste0(output_folder, "IS_peaks.csv"), 
          row.names = FALSE)
write.csv(chosen_BMIS, file = paste0(output_folder, "chosen_BMIS.csv"), 
          row.names = FALSE)
```

Check for untargeted BMIS run
```{r bmischeck}
IS_peaks <- read.csv(paste0(output_folder, "IS_peaks.csv"))

# Create plot of absolute IS areas
facet_labels <- paste(unique(IS_peaks$stan), unique(IS_peaks$feature), sep=": ")
names(facet_labels) <- unique(IS_peaks$feature)
IS_areas_gp <- IS_peaks %>%
  mutate(type=str_extract(filename, "Blk|Poo|Smp|Std")) %>%
  mutate(xax=str_extract(filename, "(?<=_.{3}_).*(?=\\.mzML)")) %>%
  # mutate(xax=paste0(str_extract(filename, "^\\d+"), "_", xax)) %>%
  mutate(xax=factor(xax, levels = unique(xax))) %>%
  ggplot() +
  geom_bar(aes(x=xax, y=M_area, fill=type), stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust=1, vjust=0.5)) +
  facet_wrap(~feature, ncol = 1, scales = "free_y",
             labeller = as_labeller(facet_labels))

ggsave(plot = IS_areas_gp, filename = paste0(output_folder, "IS_areas.pdf"), 
       device = "pdf", width = 20, height = 20, dpi = 72)


# Create BMIS plot for IS with pooled samps vs all samps
IS_pooled_cvs <- IS_peaks %>% 
  filter(str_detect(filename, "_Poo_")) %>%
  select(stan, M_area, filename) %>%
  {full_join(., ., by=c("filename"))} %>%
  group_by(stan.x, stan.y) %>%
  mutate(M_area_normed=(M_area.x/M_area.y)*mean(M_area.y)) %>%
  summarise(calc_cv=cv(M_area_normed)) %>%
  mutate(type="pooled_only")
IS_all_cvs <- IS_peaks %>% 
  select(stan, M_area, filename) %>%
  {full_join(., ., by=c("filename"))} %>%
  group_by(stan.x, stan.y) %>%
  mutate(M_area_normed=(M_area.x/M_area.y)*mean(M_area.y)) %>%
  summarise(calc_cv=cv(M_area_normed)) %>%
  mutate(type="all_files")
BMIS_on_IS_gp <- rbind(IS_pooled_cvs, IS_all_cvs) %>%
  pivot_wider(values_from = calc_cv, names_from = "type") %>%
  ggplot() +
  geom_point(aes(x=pooled_only, y=all_files, label=stan.y)) +
  facet_wrap(~stan.x, scales = "free")
ggsave(plot=BMIS_on_IS_gp, filename = paste0(output_folder, "BMIS_on_IS.pdf"), 
       device = "pdf", width = 8.5, height = 8.5)
```


## Annotate formulae here
Usually there is a Sirius section, in this case not working on Mac for now.

## Annotate classes and structures
Ditto on the Sirius section not currently working for assigning sirius IDs.
Assess classes may work once the above sections are running.

## Collect into the clean database

Isolate two most useful dataframes from above code: one to hold the values
corresponding to each *peak* and one to hold the values corresponding to each
*feature*.

```{r peakvals}
# Step 0: load relevant data
filled_peaks <- read.csv(paste0(output_folder, "filled_peaks.csv")) %>%
  select(feature, mz, rt, M_area, filename)
addiso_features <- read.csv(paste0(output_folder, "addiso_features.csv"))
chosen_BMIS <- read.csv(paste0(output_folder, "chosen_BMIS.csv"))
IS_peaks <- read.csv(paste0(output_folder, "IS_peaks.csv")) %>%
  select(compound_name, filename, is_area=M_area)

# Step 1
filled_peaks <- filled_peaks %>%
  filter(!feature%in%addiso_features$feature)

# Step 2
norm_peaks <- filled_peaks %>%
  left_join(chosen_BMIS, by="feature") %>%
  left_join(IS_peaks, by = c(BMIS="compound_name", "filename")) %>%
  group_by(feature) %>%
  mutate(norm_area=(M_area/is_area)*mean(is_area, na.rm=TRUE)) %>%
  select(feature, mz, rt, norm_area, filename)

write.csv(norm_peaks, paste0(output_folder, "norm_peaks.csv"), row.names = FALSE)
```

Featurevals then builds on this new norm_peaks data frame.
Step 1: Group by feature and produce summary statistics
 - mzmed, rtmed, avgarea
Step 2: Annotate known standards with stan_annotations
Step 3: Annotate formulae with feature_formulas
Step 4: Annotate classes with classy_confs (currently removed because annoying)
Step 5: Annotate hypothetical structures with structure_ids

```{r featurevals}
# Step 0: Load everything
stan_annotations <- read.csv(paste0(output_folder, "stan_annotations.csv")) %>%
  select(compound_name, feature)

# Step 1: group and summarize
norm_feats <- norm_peaks %>%
  filter(str_detect(filename, "_Smp_")) %>%
  group_by(feature) %>%
  summarize(mzmed=mean(mz, na.rm=TRUE), rtmed=mean(rt, na.rm=TRUE), avgarea=mean(norm_area, na.rm=TRUE))

# Step 2: authentic standards
norm_feats <- norm_feats %>%
  left_join(stan_annotations, by="feature")

# Step 3: formulae

# Step 4: classes
# 0.99 cutoff is arbitrary
if(file.exists(paste0(output_folder, "classy_confs.csv"))){
  classy_confs <- read.csv(paste0(output_folder, "classy_confs.csv"))
  clean_classes <- classy_confs %>%
    filter(conf>0.99) %>%
    group_by(feature) %>%
    summarize(classes=paste(classes, collapse = "; "))
# # Currently commented out because it's messy and super long and printing sucks
# norm_feats <- norm_feats %>%
#   left_join(clean_classes, by="feature")
  # Instead they're all being set to NA
  norm_feats$classes <- NA
} else {
  norm_feats$classes <- NA
}

# Step 5: structures
# -20 cutoff is due to Raafay's manual checks, -25 ~= 80% correct
if(file.exists(paste0(output_folder, "structure_ids.csv"))){
  classy_confs <- read.csv(paste0(output_folder, "structure_ids.csv"))
  clean_CSI <- structure_ids %>%
    filter(confidence>-25) %>%
    select(feature, CSI_name=name)
  norm_feats <- left_join(norm_feats, clean_CSI, by="feature")
} else {
  norm_feats$CSI_name <- NA
}

write.csv(norm_feats, paste0(output_folder, "norm_feats.csv"), row.names = FALSE)
```
